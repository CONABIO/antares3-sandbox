{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-10 16:28:26,276 - datacube.drivers.driver_cache - driver_cache safe_load: Failed to resolve driver datacube.plugins.index::s3aio_index\n",
      "2020-03-10 16:28:26,279 - datacube.drivers.driver_cache - driver_cache safe_load: Error was: UndefinedEnvironmentName(\"'extra' does not exist in evaluation environment.\",)\n",
      "/home/madmex_user/.local/lib/python3.6/site-packages/distributed/worker.py:3269: UserWarning: Large object of size 25.02 MB detected in task graph: \n",
      "2020-03-10 16:36:40,011 - madmex.wrappers - loggerwriter write: /home/madmex_user/.local/lib/python3.6/site-packages/distributed/worker.py:3269: UserWarning: Large object of size 25.02 MB detected in task graph: \n",
      "  (array([[False, False, False, ...,  True,  True,   ... , 3200, None)))\n",
      "2020-03-10 16:36:40,017 - madmex.wrappers - loggerwriter write:   (array([[False, False, False, ...,  True,  True,   ... , 3200, None)))\n",
      "Consider scattering large objects ahead of time\n",
      "2020-03-10 16:36:40,020 - madmex.wrappers - loggerwriter write: Consider scattering large objects ahead of time\n",
      "with client.scatter to reduce scheduler burden and \n",
      "2020-03-10 16:36:40,023 - madmex.wrappers - loggerwriter write: with client.scatter to reduce scheduler burden and \n",
      "keep data on workers\n",
      "2020-03-10 16:36:40,027 - madmex.wrappers - loggerwriter write: keep data on workers\n",
      "\n",
      "2020-03-10 16:36:40,030 - madmex.wrappers - loggerwriter write: \n",
      "    future = client.submit(func, big_data)    # bad\n",
      "2020-03-10 16:36:40,034 - madmex.wrappers - loggerwriter write:     future = client.submit(func, big_data)    # bad\n",
      "\n",
      "2020-03-10 16:36:40,038 - madmex.wrappers - loggerwriter write: \n",
      "    big_future = client.scatter(big_data)     # good\n",
      "2020-03-10 16:36:40,040 - madmex.wrappers - loggerwriter write:     big_future = client.scatter(big_data)     # good\n",
      "    future = client.submit(func, big_future)  # good\n",
      "2020-03-10 16:36:40,043 - madmex.wrappers - loggerwriter write:     future = client.submit(func, big_future)  # good\n",
      "  % (format_bytes(len(b)), s)\n",
      "2020-03-10 16:36:40,047 - madmex.wrappers - loggerwriter write:   % (format_bytes(len(b)), s)\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import datetime as dt\n",
    "import os\n",
    "from operator import itemgetter\n",
    "import shutil \n",
    "import pickle\n",
    "import time\n",
    "import json\n",
    "\n",
    "from django.contrib.gis.geos.geometry import GEOSGeometry\n",
    "from osgeo import osr\n",
    "from osgeo import gdal\n",
    "from pyproj import Proj\n",
    "from fiona.crs import from_epsg\n",
    "from fiona.crs import to_string\n",
    "from sklearn.cluster import KMeans\n",
    "import geopandas as gpd\n",
    "from affine import Affine\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from rasterio.features import rasterize\n",
    "from shapely.geometry import shape\n",
    "import fiona\n",
    "import glob\n",
    "from dask.distributed import Client\n",
    "from datacube.drivers.netcdf import write_dataset_to_netcdf\n",
    "from datacube.api import GridWorkflow\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "\n",
    "from madmex.wrappers import gwf_query\n",
    "from madmex.util.xarray import to_float, to_int\n",
    "from madmex.models import PredictObject, TrainClassification\n",
    "from madmex.models import Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=5,memory_limit='120GB', threads_per_worker=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:36625</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>5</li>\n",
       "  <li><b>Cores: </b>5</li>\n",
       "  <li><b>Memory: </b>600.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:36625' processes=5 threads=5, memory=600.00 GB>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:36625</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>5</li>\n",
       "  <li><b>Cores: </b>5</li>\n",
       "  <li><b>Memory: </b>600.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:36625' processes=5 threads=5, memory=600.00 GB>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'cultivos_2019_simplify'\n",
    "name_of_product = 's2_l2a_10m_scl_mexico'\n",
    "begin = '2018-01-01'\n",
    "end = '2019-12-31'\n",
    "gwf_kwargs = {'region': region, 'product': name_of_product, 'begin': begin, 'end':end}\n",
    "iterable = gwf_query(**gwf_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_iter = list(iterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lista_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_today = datetime.today().strftime(\"%m-%d-%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_today='03-04-2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'03-04-2020'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_file='/LUSTRE/MADMEX/tasks/2020/1_clusterization_for_agriculture_labeling/madmex_masks_agricultura_pastizal/madmex_mascara_s2_2018_agricultura_pastizal_Veracruz_una_clase.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "crs_dict = {'proj': 'lcc',\n",
    " 'lat_1': 17.5,\n",
    " 'lat_2': 29.5,\n",
    " 'lat_0': 12,\n",
    " 'lon_0': -102,\n",
    " 'x_0': 2500000,\n",
    " 'y_0': 0,\n",
    " 'datum': 'WGS84',\n",
    " 'units': 'm',\n",
    " 'no_defs': True,\n",
    " 'a':6378137,\n",
    " 'b':6378136.027241431,\n",
    " 'wktext': True}\n",
    "#+ellps=WGS84 +datum=WGS84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = 'cultivos2019_inecol'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/LUSTRE/MADMEX/tasks/2020/1_clusterization_for_agriculture_labeling/features_computed/all_dc_tiles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, (47, -35)),\n",
       " (1, (47, -37)),\n",
       " (2, (48, -37)),\n",
       " (3, (47, -36)),\n",
       " (4, (48, -36)),\n",
       " (5, (40, -31)),\n",
       " (6, (39, -31)),\n",
       " (7, (38, -30)),\n",
       " (8, (39, -30)),\n",
       " (9, (40, -30)),\n",
       " (10, (39, -29)),\n",
       " (11, (38, -29)),\n",
       " (12, (40, -29)),\n",
       " (13, (38, -28)),\n",
       " (14, (40, -28)),\n",
       " (15, (38, -27)),\n",
       " (16, (39, -28)),\n",
       " (17, (39, -27)),\n",
       " (18, (38, -26)),\n",
       " (19, (39, -26)),\n",
       " (20, (40, -33)),\n",
       " (21, (40, -32)),\n",
       " (22, (39, -32)),\n",
       " (23, (37, -29)),\n",
       " (24, (40, -34)),\n",
       " (25, (41, -33)),\n",
       " (26, (42, -33)),\n",
       " (27, (41, -32)),\n",
       " (28, (42, -32)),\n",
       " (29, (41, -31)),\n",
       " (30, (42, -31)),\n",
       " (31, (43, -33)),\n",
       " (32, (44, -34)),\n",
       " (33, (43, -35)),\n",
       " (34, (45, -34)),\n",
       " (35, (44, -35)),\n",
       " (36, (45, -37)),\n",
       " (37, (43, -34)),\n",
       " (38, (44, -37)),\n",
       " (39, (43, -36)),\n",
       " (40, (45, -36)),\n",
       " (41, (45, -35)),\n",
       " (42, (44, -36)),\n",
       " (43, (42, -34)),\n",
       " (44, (41, -34)),\n",
       " (45, (37, -27)),\n",
       " (46, (37, -26)),\n",
       " (47, (37, -28)),\n",
       " (48, (41, -30)),\n",
       " (49, (46, -35)),\n",
       " (50, (46, -34)),\n",
       " (51, (46, -36)),\n",
       " (52, (46, -37))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k,lista_iter[k][0]) for k in range(0,len(lista_iter))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "cannot convert float infinity to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0mTraceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/madmex_user/.local/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"/home/madmex_user/.local/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/home/madmex_user/.local/lib/python3.6/site-packages/joblib/_parallel_backends.py\", line 600, in __call__\n    return self.func(*args, **kwargs)\n  File \"/home/madmex_user/.local/lib/python3.6/site-packages/joblib/parallel.py\", line 256, in __call__\n    for func, args, kwargs in self.items]\n  File \"/home/madmex_user/.local/lib/python3.6/site-packages/joblib/parallel.py\", line 256, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"/home/madmex_user/.local/lib/python3.6/site-packages/sklearn/cluster/_k_means.py\", line 314, in _kmeans_single_elkan\n    x_squared_norms=x_squared_norms)\n  File \"/home/madmex_user/.local/lib/python3.6/site-packages/sklearn/cluster/_k_means.py\", line 626, in _init_centroids\n    x_squared_norms=x_squared_norms)\n  File \"/home/madmex_user/.local/lib/python3.6/site-packages/sklearn/cluster/_k_means.py\", line 88, in _k_init\n    n_local_trials = 2 + int(np.log(n_clusters))\nOverflowError: cannot convert float infinity to integer\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOverflowError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d286f7bcc77d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnclusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfeatures_for_kmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgdf_df_join_reduced2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'geometry'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'features_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgdf_df_join_reduced2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures_for_kmeans\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0mmodel_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_result_dc_tile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdate_today\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_model_'\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0mdc_tile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_nclusters_%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnclusters\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_aoi'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/cluster/_k_means.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    953\u001b[0m                     \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m                 )\n\u001b[0;32m--> 955\u001b[0;31m                 for seed in seeds)\n\u001b[0m\u001b[1;32m    956\u001b[0m             \u001b[0;31m# Get results with the lowest inertia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minertia\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    906\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    552\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    553\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOverflowError\u001b[0m: cannot convert float infinity to integer"
     ]
    }
   ],
   "source": [
    "for var in lista_iter[23:24]:\n",
    "    dc_tile = '%d_%d' % (var[0][0], var[0][1])\n",
    "    control = 'processing_' + dc_tile + '.txt'\n",
    "    file_control = open(path+control,'w+')\n",
    "    file_control.write('Gridworkflow load\\n')\n",
    "    sr_0 = GridWorkflow.load(var[1],dask_chunks={'x': 1600, 'y': 1600,'time': 60})\n",
    "    sr_0.attrs['geobox'] = var[1].geobox\n",
    "    sr_0 = client.persist(sr_0)\n",
    "    sr_0 = sr_0.apply(func=to_float, keep_attrs=True)\n",
    "    sr_1 = sr_0.where(sr_0.pixel_qa.isin([2,4,5,6,7,8,11]))\n",
    "    sr_1['ndvi'] = ((sr_1.nir - sr_1.red) / (sr_1.nir + sr_1.red)) * 10000\n",
    "    sr_1['ndvi'].attrs['nodata'] = 0\n",
    "    sr_1['gndvi'] = ((sr_1.nir - sr_1.green) / (sr_1.nir + sr_1.green)) * 10000\n",
    "    sr_1['gndvi'].attrs['nodata'] = 0\n",
    "    sr_1['avi'] = ((sr_1.nir*(1-sr_1.red))*(sr_1.nir-sr_1.red))**(1/3)\n",
    "    sr_1['avi'].attrs['nodata'] = 0\n",
    "    sr_1 = client.persist(sr_1)\n",
    "    file_control.write('finished features computation\\n')\n",
    "    ndvi_mean_resampled = sr_1.ndvi.resample(time='M').mean()\n",
    "    ndvi_mean_resampled = client.persist(ndvi_mean_resampled)\n",
    "    ndvi_mean_resampled_drop_na = ndvi_mean_resampled.dropna('time',how='all')\n",
    "    gndvi_mean_resampled = sr_1.gndvi.resample(time='M').mean()\n",
    "    gndvi_mean_resampled = client.persist(gndvi_mean_resampled)\n",
    "    gndvi_mean_resampled_drop_na = gndvi_mean_resampled.dropna('time',how='all')\n",
    "    avi_mean_resampled = sr_1.avi.resample(time='M').mean()\n",
    "    avi_mean_resampled = client.persist(avi_mean_resampled)\n",
    "    avi_mean_resampled_drop_na = avi_mean_resampled.dropna('time',how='all')\n",
    "    combined = xr.merge([ndvi_mean_resampled_drop_na,\n",
    "                     gndvi_mean_resampled_drop_na,\n",
    "                     avi_mean_resampled_drop_na])\n",
    "    combined = client.persist(combined)\n",
    "    features = list(combined.data_vars)\n",
    "    s = '-'\n",
    "    features_string = s.join(features)\n",
    "    path_result = path + features_string + '/'\n",
    "    path_result_dc_tile = os.path.join(path_result, date_today \n",
    "                                   + '/' + dc_tile + '/')\n",
    "    if not os.path.exists(path_result_dc_tile):\n",
    "        os.makedirs(path_result_dc_tile)\n",
    "    combined.attrs = sr_0.attrs\n",
    "    combined.coords['time'].attrs = sr_0.coords['time'].attrs\n",
    "    combined.coords['x'].attrs = sr_0.coords['x'].attrs\n",
    "    combined.coords['y'].attrs = sr_0.coords['y'].attrs\n",
    "    file_control.write('beginning reading of segmentation\\n')\n",
    "    #read segmentation\n",
    "    geom = GEOSGeometry(json.dumps(sr_0.geobox.geographic_extent.json))\n",
    "    seg_name = 'seg_mex_s2_10m_scl_2018_2019'\n",
    "    qs = PredictObject.objects.filter(the_geom__contained=geom,segmentation_information__name=seg_name)\n",
    "    path_seg = qs[0].path\n",
    "    with fiona.open(path_seg) as src:\n",
    "        fc = list(src)\n",
    "        crs = to_string(src.crs)\n",
    "    fc_subset=((x['properties']['id'], x['geometry']) for x in fc)\n",
    "    fc_sorted = sorted(fc_subset, key=itemgetter(0))\n",
    "    iterable_zip = zip([k[1] for k in fc_sorted], [k[0] for k in fc_sorted])\n",
    "    aff = Affine(*list(sr_0.affine)[0:6])\n",
    "    dimensions_dataset = list(sr_0.coords)\n",
    "    list_dimensions = [x for x in dimensions_dataset if x != 'time']\n",
    "    lambda_function = lambda l_netcdf,l_test: l_netcdf[0] if l_netcdf[0] in l_test else l_netcdf[1]\n",
    "    xdim = lambda_function(list_dimensions,['x','longitude'])\n",
    "    ydim = lambda_function(list_dimensions,['y','latitude'])\n",
    "    fc_raster = rasterize(iterable_zip, transform=aff,\n",
    "                          out_shape=(sr_0.sizes[ydim], sr_0.sizes[xdim]),\n",
    "                          dtype='float64', fill=np.nan)\n",
    "    fc_raster = fc_raster.astype(int)\n",
    "    fc_dataarray = xr.DataArray(fc_raster, dims=[ydim, xdim], name='features_id')\n",
    "    file_control.write('beginning masking\\n')\n",
    "    geometry_extent = sr_0.geobox.extent.json\n",
    "    geom_extent_schema={'properties':{'id':'int'},'geometry': 'Polygon'}\n",
    "    layer_name = 'geom_extent_' + dc_tile\n",
    "    geom_extent_file = path_result + '/' + date_today + '/' + dc_tile + '/' + layer_name + '.shp'\n",
    "    with fiona.open(geom_extent_file,'w',\n",
    "                driver='ESRI Shapefile',\n",
    "                crs=crs_dict,\n",
    "                layer=layer_name,\n",
    "                schema=geom_extent_schema) as dst:\n",
    "        dst.write({'properties':{'id':0},\n",
    "                   'geometry': geometry_extent})\n",
    "    mask_crop = gdal.Warp('', mask_file, format = 'VRT',\n",
    "                          cutlineDSName=geom_extent_file,\n",
    "                          cutlineLayer = layer_name,\n",
    "                          cropToCutline=True)\n",
    "    arr=mask_crop.ReadAsArray()\n",
    "    combined_masked = combined.where(arr==1)\n",
    "    file_control.write('finished masking\\n')\n",
    "    combined = xr.merge([fc_dataarray,combined_masked])\n",
    "    combined = client.persist(combined)\n",
    "    combined = combined.stack(z=(xdim, ydim))\n",
    "    combined = combined.reset_index('z')\n",
    "    combined = combined.drop([xdim,ydim])\n",
    "    combined = combined.where((np.isfinite(combined['features_id'])), drop=True)\n",
    "    combined = client.persist(combined)\n",
    "    file_control.write('beginning dataframe construction\\n')\n",
    "    df = combined.to_dataframe()\n",
    "    del sr_0\n",
    "    del ndvi_mean_resampled_drop_na\n",
    "    del gndvi_mean_resampled_drop_na\n",
    "    del avi_mean_resampled_drop_na\n",
    "    file_control.write('restarting client\\n')\n",
    "    client.restart()\n",
    "    df=df.reset_index('time')\n",
    "    df2 = df.groupby(['time','features_id'],as_index=False).agg({features[0]: 'median',\n",
    "                                                                 features[1]: 'median',\n",
    "                                                                 features[2]: 'median'})\n",
    "    df2.index = df2['time']\n",
    "    df2.drop(['time'], axis=1, inplace=True)\n",
    "    times_df = [datetime.date(d).strftime('%Y-%m-%d') for d in df2.index.get_level_values('time').unique()]\n",
    "    df3 = df2.loc[times_df[0]]\n",
    "    df3.rename(columns={features[0]: features[0]+'_' + times_df[0],\n",
    "                        features[1]: features[1]+'_' +times_df[0],\n",
    "                        features[2]: features[2]+'_' +times_df[0]},\n",
    "               inplace=True)\n",
    "    df3=df3.reset_index('time')\n",
    "    df3.drop(['time'],axis=1,inplace=True)\n",
    "    for k in range(1,len(times_df)):\n",
    "        df_subset = df2.loc[times_df[k]]\n",
    "        df_subset.dropna(how='all', inplace=True)\n",
    "        df_subset.rename(columns={features[0]: features[0]+'_' + times_df[k],\n",
    "                                  features[1]: features[1]+'_' +times_df[k],\n",
    "                                  features[2]: features[2]+'_' +times_df[k]},\n",
    "                         inplace=True)\n",
    "        df_subset=df_subset.reset_index('time')\n",
    "        df_subset.drop(['time'],axis=1,inplace=True)\n",
    "        df3 = df3.join(df_subset.set_index('features_id'), on='features_id') \n",
    "    file_control.write('finished building of df3\\n')\n",
    "    os.remove(geom_extent_file)\n",
    "    os.remove(path_result + '/' + date_today + '/' + dc_tile + '/' + layer_name + '.dbf')\n",
    "    os.remove(path_result + '/' + date_today + '/' + dc_tile + '/' + layer_name + '.shx')\n",
    "    os.remove(path_result + '/' + date_today + '/' + dc_tile + '/' + layer_name + '.cpg')\n",
    "    os.remove(path_result + '/' + date_today + '/' + dc_tile + '/' + layer_name + '.prj')\n",
    "    file_control.write('beginning building of gdf\\n')\n",
    "    gdf = gpd.GeoDataFrame.from_features([feature for feature in fc], \n",
    "                                          crs = crs_dict)\n",
    "    gdf = gdf.rename(columns={'id':'features_id'})\n",
    "    gdf_df_join = gdf.merge(df3, on='features_id', how='left')\n",
    "    gdf_df_join_reduced = gdf_df_join.drop(['features_id','geometry'],axis=1)\n",
    "    gdf_df_join_reduced.dropna(how='all', inplace=True)\n",
    "    t_keep_rows = gdf_df_join_reduced.index.values\n",
    "    gdf_df_join_reduced2 = gdf_df_join.loc[gdf_df_join.index.isin(t_keep_rows)]\n",
    "    file_control.write('beginning unsupervised classif\\n')\n",
    "    qs_t = TrainClassification.objects.filter(train_object__the_geom__contained=geom,\n",
    "                                              training_set=training_set).prefetch_related('interpret_tag')\n",
    "    l_tag_ids = []\n",
    "    for element in qs_t:\n",
    "        l_tag_ids.append(element.interpret_tag.id)\n",
    "    nclusters = len(np.unique(l_tag_ids))\n",
    "    kmeans = KMeans(n_clusters=nclusters, n_jobs=10)\n",
    "    features_for_kmeans = [column for column in list(gdf_df_join_reduced2.columns) if column not in ['geometry','features_id']]\n",
    "    kmeans.fit(gdf_df_join_reduced2[features_for_kmeans].fillna(0).values)\n",
    "    model_filename = path_result_dc_tile + date_today + '_model_' +  dc_tile + '_nclusters_%d' % nclusters + '_aoi' + '.pkl'\n",
    "    pickle.dump(kmeans, open(model_filename, \"wb\"))\n",
    "    preds = kmeans.predict(gdf_df_join_reduced2[features_for_kmeans].fillna(0).values)\n",
    "    gdf_df_clusters = gdf_df_join_reduced2.drop(features_for_kmeans,axis=1)\n",
    "    gdf_df_clusters['preds'] = preds\n",
    "    file_control.write('writing shapefile\\n')\n",
    "    clusters_filename = path_result_dc_tile + date_today + '_clusters_' +  dc_tile + '_nclusters_%d' % nclusters + '_aoi2' + '.shp'\n",
    "    gdf_df_clusters.crs = crs_dict\n",
    "    gdf_df_clusters.to_file(clusters_filename)\n",
    "    file_control.write('finished processing\\n')\n",
    "    file_control.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
