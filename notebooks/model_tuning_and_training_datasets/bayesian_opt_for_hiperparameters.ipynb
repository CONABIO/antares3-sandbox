{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import logging\n",
    "import logging.config\n",
    "logger = logging.getLogger()\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "fh = logging.FileHandler('bayesian_opt.log') ##### change the log file name!\n",
    "fh.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.metrics import roc_curve, auc, classification_report\n",
    "from sklearn.metrics import precision_score as user_acc\n",
    "from sklearn.metrics import recall_score as prod_acc\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, log_loss\n",
    "\n",
    "from madmex.validation import validate, pprint_val_dict\n",
    "\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_file = '/shared_volume/datacube/training_objects/training_mex_landsat_2017_002_L7L8_1415.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ds_file, 'rb') as f:\n",
    "    training_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4327748,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data per class. Classes number correspond to id field in madmex_tag table, not to numeric_code filed!!\n",
    "pxpcl = np.array([training_dataset[1][training_dataset[1] == cl].size for cl in range(1,32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3075, 391632, 360178,  41292,  66594,  44811, 141600,  22293,\n",
       "        76187,   2410, 390418, 119866, 561611,  22691,  86128,  43360,\n",
       "       136050,  12731,   8686, 213173,   1576,  20744,   1906,  41933,\n",
       "        15784,  40880, 523000, 763402,  56011,  43334,  74392])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pxpcl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modification of remove_outliers method to also give as output the outliers, in case we want to explore them\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from madmex.util.numpy import groupby\n",
    "\n",
    "def remove_outliers(X, y, n_estimators=101, max_samples='auto', contamination=0.25,\n",
    "                        bootstrap=True, n_jobs=-1, **kwargs):\n",
    "        \"\"\"Performs outliers detection and removal using Isolation Forest anomaly score\n",
    "        Args:\n",
    "            X (np.ndarray): Array of independent variables of shape (n,m)\n",
    "            y (np.ndarray): Array of dependent variable of shape (n,)\n",
    "            contamination (float): The amount of contamination of the data set,\n",
    "                i.e. the proportion of outliers in the data set. Used when\n",
    "                fitting to define the threshold on the decision function.\n",
    "            max_sample (float): Proportion of observations to draw from X to fit\n",
    "                each estimator\n",
    "            **kwargs: Arguments passed to ``sklearn.ensemble.IsolationForest``\n",
    "        Example:\n",
    "            >>> from sklearn.datasets import make_classification\n",
    "            >>> from madmex.modeling import BaseModel\n",
    "            >>> X, y = make_classification(n_samples=10000, n_features=10,\n",
    "            >>>                            n_classes=5, n_informative=6)\n",
    "            >>> X_clean, y_clean = BaseModel.remove_outliers(X, y)\n",
    "            >>> print('Input shape:', X.shape, 'Output shape:', X_clean.shape)\n",
    "        Return:\n",
    "            tuple: Tuple of filtered X and y arrays (X, y)\n",
    "        \"\"\"\n",
    "        # Split X\n",
    "        grouped = groupby(X, y)\n",
    "        X_list = []\n",
    "        y_list = []\n",
    "        Xo_list = []\n",
    "        yo_list = []\n",
    "        for g in grouped:\n",
    "            isolation_forest = IsolationForest(n_estimators=n_estimators,\n",
    "                                               max_samples=max_samples,\n",
    "                                               contamination=contamination,\n",
    "                                               bootstrap=bootstrap,\n",
    "                                               n_jobs=n_jobs,\n",
    "                                               **kwargs)\n",
    "            isolation_forest.fit(g[1])\n",
    "            is_inlier = isolation_forest.predict(g[1])\n",
    "            is_outlier = np.where(is_inlier == 1, False, True)\n",
    "            is_inlier = np.where(is_inlier == 1, True, False)\n",
    "            X_out = g[1][is_inlier,:]\n",
    "            X_list.append(X_out)\n",
    "            Xo_out = g[1][is_outlier,:]\n",
    "            Xo_list.append(Xo_out)\n",
    "\n",
    "            y_out = np.empty_like(X_out[:,0], dtype=np.int16)\n",
    "            y_out[:] = g[0]\n",
    "            y_list.append(y_out)\n",
    "            yo_out = np.empty_like(Xo_out[:,0], dtype=np.int16)\n",
    "            yo_out[:] = g[0]\n",
    "            yo_list.append(yo_out)\n",
    "        # Concatenate returned arrays\n",
    "        X = np.concatenate(X_list)\n",
    "        y = np.concatenate(y_list)\n",
    "        Xo = np.concatenate(Xo_list)\n",
    "        yo = np.concatenate(yo_list)\n",
    "        return (X, y, Xo, yo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y is the data without outliers and Xo, yo are the outliers\n",
    "X, y, Xo, yo = remove_outliers(training_dataset[0],training_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pxpcl_ro = np.array([y[y == cl].size for cl in range(1,32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pxpcl_o = np.array([yo[yo == cl].size for cl in range(1,32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3075, 391632, 360178,  41292,  66594,  44811, 141600,  22293,\n",
       "        76187,   2410, 390418, 119866, 561611,  22691,  86128,  43360,\n",
       "       136050,  12731,   8686, 213173,   1576,  20744,   1906,  41933,\n",
       "        15784,  40880, 523000, 763402,  56011,  43334,  74392])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pxpcl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2306, 293724, 270133,  30969,  49945,  33608, 106200,  16720,\n",
       "        57140,   1807, 292813,  89899, 421208,  17018,  64596,  32520,\n",
       "       102037,   9548,   6514, 159880,   1182,  15558,   1429,  31450,\n",
       "        11838,  30660, 392250, 572551,  42008,  32500,  55794])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pxpcl_ro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   769,  97908,  90045,  10323,  16649,  11203,  35400,   5573,\n",
       "        19047,    603,  97605,  29967, 140403,   5673,  21532,  10840,\n",
       "        34013,   3183,   2172,  53293,    394,   5186,    477,  10483,\n",
       "         3946,  10220, 130750, 190851,  14003,  10834,  18598])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pxpcl_o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split dataset\n",
    "to have a subsample of X, y we take 20000 per class and in case a class has less than 20000, the whole data of that class is taken. We shuffle the data, because X,y is ordered and that is not convenient for training sake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = [np.where(y== cl)[0] for cl in range(1,32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2306"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indexes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffled(l):\n",
    "    o = []\n",
    "    _ = [o.append(i) for i in l]\n",
    "    random.shuffle(o)\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(35473234)\n",
    "new_indexes = [ shuffled(indexes[cl])[:20000] if len(indexes[cl]) >= 20000 else shuffled(indexes[cl]) for cl in range(31) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15558"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_indexes[21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_new_indexes = np.array(np.concatenate([new_indexes[0],new_indexes[1],new_indexes[2],new_indexes[3],new_indexes[4],\n",
    "                                              new_indexes[5],new_indexes[6],new_indexes[7],new_indexes[8],new_indexes[9],\n",
    "                                              new_indexes[10],new_indexes[11],new_indexes[12],new_indexes[13],\n",
    "                                              new_indexes[14],new_indexes[15],new_indexes[16],new_indexes[17],\n",
    "                                              new_indexes[18],new_indexes[19],new_indexes[20],new_indexes[21],\n",
    "                                              new_indexes[22],new_indexes[23],new_indexes[24],new_indexes[25],\n",
    "                                              new_indexes[26],new_indexes[27],new_indexes[28],new_indexes[29],\n",
    "                                              new_indexes[30]]),int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "503920"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_new_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = X[merged_new_indexes,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y = y[merged_new_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(503920, 15)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(503920,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the new subset into training and test datasets, where the test dataset is the 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_X, new_y, test_size=0.2, random_state=546842346)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_shuff, y_shuff = shuffle(X_train,y_train,random_state=658432434)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data per class of final training dataset\n",
    "pxpcl_tr = np.array([y_train[y_train == cl].size for cl in range(1,32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1847, 16082, 15960, 16015, 16092, 15900, 15984, 13385, 15998,\n",
       "        1460, 16002, 16040, 15970, 13560, 15989, 15894, 16007,  7615,\n",
       "        5134, 16080,   954, 12512,  1174, 15989,  9442, 15950, 15992,\n",
       "       16015, 16129, 15951, 16014])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pxpcl_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New hyperparameters with bayesian optimization for lgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = [Real(0.1, 0.5, name='reg_lambda'),\n",
    "                Real(0.1, 0.5, name='reg_alpha'),\n",
    "                Integer(500, 1500, name='n_estimators'),\n",
    "                Integer(5, 25, name='max_depth'),\n",
    "                Real(0.001,0.1, name='learning_rate')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BayesianOptimization(values):\n",
    "    params = {'reg_lambda': values[0], 'reg_alpha': values[1], 'random_state': 234567943, 'n_jobs': 5, \n",
    "              'n_estimators': values[2], 'max_depth': values[3], 'learning_rate': values[4]}\n",
    "\n",
    "    print('\\nTesting next set of paramaters...', params)\n",
    "    logger.debug('\\nTesting next set of paramaters... {}'.format(params))\n",
    "    \n",
    "    modeloLGB = LGBMClassifier(**params)\n",
    "    modeloLGB.fit(X_shuff,y_shuff)\n",
    "    \n",
    "    y_pred_test_lgb = modeloLGB.predict(X_test)\n",
    "    y_pred_prob_test_lgb = modeloLGB.predict_proba(X_test)\n",
    "    \n",
    "    classification_report(y_pred_test_lgb, y_test)\n",
    "    acc = accuracy_score(y_test, y_pred_test_lgb)\n",
    "    print(\"Test accuracy: {}\".format(acc))\n",
    "    logger.debug(\"Test accuracy: {}\".format(acc))\n",
    "    \n",
    "    print(\"Test log loss: {}\".format(log_loss(y_test, y_pred_prob_test_lgb)))\n",
    "    logger.debug(\"Test log loss: {}\".format(log_loss(y_test, y_pred_prob_test_lgb)))\n",
    "    \n",
    "    y_pred_shuff_lgb = modeloLGB.predict(X_shuff)\n",
    "    y_pred_prob_shuff_lgb = modeloLGB.predict_proba(X_shuff)\n",
    "    \n",
    "    classification_report(y_pred_shuff_lgb, y_shuff)\n",
    "    print(\"Training accuracy: {}\".format(accuracy_score(y_shuff, y_pred_shuff_lgb)))\n",
    "    logger.debug(\"Training accuracy: {}\".format(accuracy_score(y_shuff, y_pred_shuff_lgb)))\n",
    "    print(\"Training log loss: {}\".format(log_loss(y_shuff, y_pred_prob_shuff_lgb)))\n",
    "    logger.debug(\"Training log loss: {}\".format(log_loss(y_shuff, y_pred_prob_shuff_lgb)))\n",
    "    \n",
    "    return -acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'reg_lambda': 0.25514521380251476, 'reg_alpha': 0.24878301265635247, 'random_state': 234567943, 'n_jobs': 5, 'n_estimators': 1288, 'max_depth': 17, 'learning_rate': 0.04637138312542715}\n"
     ]
    }
   ],
   "source": [
    "res_gp = gp_minimize(BayesianOptimization, search_space, random_state=23471416, n_jobs=5, verbose=True, n_random_starts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
